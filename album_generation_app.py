##############################################################
#                                                            #
#           Album Generation App from user's prompt          #
#                                                            #
##############################################################

import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import ChatPromptTemplate
from langchain.schema import BaseOutputParser
from typing import List
from langchain.utilities.dalle_image_generator import DallEAPIWrapper


class ParseOutput(BaseOutputParser[List[str]]):
    def parse(self, text: str) -> List[str]:
        return text

st.title('Album Generator')

# Define sidebar variables
openai_api_key = st.sidebar.text_input('OpenAI API Key')
songs_number = st.sidebar.text_input('How many songs?')
temperature = st.sidebar.slider('Select a song', 0, 8, 3)


# Define the health advisor function. It accepts only the input text and outputs all necessary information about an album cover (Name, Theme, Image and lyrics)
# It also uses the vector store to store information about the conversation, therefore as long as the session storage is not cleared, the conversation will continue from where it was left off.

# The flow of the model is the following:
# 1. The user enters a prompt for the album
# 2. The model generates an album theme based on the user prompt
# 3. The model generates an album cover image based on the album theme
# 4. The model generates songs based on the album theme
# 5. The model outputs the album theme, the album cover image and the songs
def health_advisor(input_text, vector_store):
    # Define the model that will generate the theme based on the user prompt
    # I'm using the gpt-3.5-turbo-1106 model, as it is the best available model for generating text (and cheap also)
    # the temperature of the model is controlled from the sidebar slider
    theme_generator_model = ChatOpenAI(openai_api_key=openai_api_key, temperature=temperature, model='gpt-3.5-turbo-1106')
    template = "You are an AI album generator. You generate an album theme based on the user prompt. This theme will be used for generating the album cover image and the album name, and will influence the lyrics."
    vector_store.setdefault('album_generator_history', []).append(f"User's prompt is {input_text}")

    # Construct the chat prompt with vector store information
    chat_prompt = ChatPromptTemplate.from_messages([
        template,
        *vector_store.get('album_generator_history')
    ])

    # Invoke the model chain
    # Chat Prompt -> Theme Generator Model -> Parse Output
    chain = chat_prompt | theme_generator_model | ParseOutput()
    response = chain.invoke(vector_store)

    # Define the model that will generate the album cover image based on the album theme
    # First I define the model that will create the prompt based on the theme. It is the most optimal way to generate the album cover image
    album_cover_prompt_generator = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.8, model='gpt-3.5-turbo-1106')
    template_album_cover = f"You are an AI album cover generator. You generate an album cover image based on the album theme. The album theme is: {response}."
    album_cover_prompt = ChatPromptTemplate.from_messages([
        template_album_cover,
        f"Generate an album cover image based on the album theme."
    ])
    # Album Theme Chat Prompt -> Album Cover Prompt Generator -> Parse Output
    album_cover_chain = album_cover_prompt | album_cover_prompt_generator | ParseOutput()
    album_cover_response = album_cover_chain.invoke(vector_store)

    # Finally I define the model that will generate the album cover image based on the prompt generated by the previous model
    # For some reason using directly the api with model dall-e-2, I got this error: "You are not allowed to sample from this model"
    # For that I'm using the DallEAPIWrapper class, which is a wrapper around the Dall-E API.
    image_url = DallEAPIWrapper(openai_api_key=openai_api_key, size="512x512").run(album_cover_response)

    # Define the model that will generate the songs based on the album theme
    songs_generator_model = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.8, model='gpt-3.5-turbo-1106')
    template_songs = f"You are an AI songs generator. You generate songs based on the album theme. You generate the lyrics only based on the album theme: {response}."

    # Construct the chat prompt with vector store information
    chat_prompt_songs = ChatPromptTemplate.from_messages([
        template_songs,
        f"Generate {songs_number} songs."
    ])

    vector_store.setdefault('album_generator_history', []).append(f"User's prompt is {input_text}")
    # Album theme chat prompt -> Songs Generator Model -> Print Songs
    chain_songs = chat_prompt_songs | songs_generator_model | ParseOutput()
    response_songs = chain_songs.invoke(vector_store)

    # Now write the output to the streamlit app
    st.markdown(f"![Album Cover Image]({image_url})") 
    st.header('Album Theme')
    st.info(response, icon='ðŸ”¥')
    st.header('Album Songs')
    st.info(response_songs)


with st.form('album_form'):
    text = st.text_area('Enter your prompt for your album.')
    submitted = st.form_submit_button('Submit')

    # Initialize or retrieve vector store
    vector_store = st.session_state.get('album_generator_store', {})

    if submitted and openai_api_key.startswith('sk-'):
        health_advisor(text, vector_store)
        # Update the vector store for future conversations
        st.session_state.album_generator_store = vector_store
    elif not openai_api_key.startswith('sk-'):
        st.warning('Please enter your OpenAI API key!', icon='âš ')

















#    vector_store.setdefault('message_history', []).append(f"User's input: {input_text}")

#     # Construct the chat prompt with vector store information
#     chat_prompt = ChatPromptTemplate.from_messages([
#         template,
#         *vector_store.get('message_history')
#     ])

#     # Update vector store with new information
#     vector_store['input_text'] = input_text

#     # Invoke the model chain
#     chain = chat_prompt | chat_model | ParseOutput()
#     response = chain.invoke(vector_store)

#     vector_store.setdefault('message_history', []).append(f"Model's response: {response}")

#     st.session_state.meal_vector_store = vector_store